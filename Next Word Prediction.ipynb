{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba21ccf",
   "metadata": {},
   "source": [
    "# Next Word Prediction using Tenserflow and Keras Library\n",
    "# Author : Nischitha D\n",
    "# Data Science Intern@Letsgrowmore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d2fa3",
   "metadata": {},
   "source": [
    "# Import Iibraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89dfbe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b014997f",
   "metadata": {},
   "source": [
    "# Load the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f363644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1661-0.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4515b6",
   "metadata": {},
   "source": [
    "# Preprocessing the text , Convert to lowercase and split into words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3477cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b35697",
   "metadata": {},
   "source": [
    "# Creating tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c8dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620293e",
   "metadata": {},
   "source": [
    "# Creating input sequences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac36896",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for i in range(1, len(corpus)):\n",
    "    sequence = corpus[i-1:i+1]\n",
    "    input_sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20237c",
   "metadata": {},
   "source": [
    "# Convert input sequences to numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd14b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046da29b",
   "metadata": {},
   "source": [
    "#  Pad sequences to have consistent length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1598f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25ce47",
   "metadata": {},
   "source": [
    "#  Split sequences into input (X) and label (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0da4336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:, 0]\n",
    "y = sequences[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63fcf0",
   "metadata": {},
   "source": [
    "# Building RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b21542fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
    "    tf.keras.layers.SimpleRNN(150),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4d832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de3a58",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497952dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3363/3363 [==============================] - 101s 30ms/step - loss: 5.8816 - accuracy: 0.1022\n",
      "Epoch 2/50\n",
      "3363/3363 [==============================] - 98s 29ms/step - loss: 5.2668 - accuracy: 0.1366\n",
      "Epoch 3/50\n",
      "3363/3363 [==============================] - 104s 31ms/step - loss: 5.0433 - accuracy: 0.1536\n",
      "Epoch 4/50\n",
      "3363/3363 [==============================] - 105s 31ms/step - loss: 4.8795 - accuracy: 0.1656\n",
      "Epoch 5/50\n",
      "3363/3363 [==============================] - 104s 31ms/step - loss: 4.7503 - accuracy: 0.1708\n",
      "Epoch 6/50\n",
      "3363/3363 [==============================] - 104s 31ms/step - loss: 4.6488 - accuracy: 0.1753\n",
      "Epoch 7/50\n",
      "3363/3363 [==============================] - 103s 31ms/step - loss: 4.5701 - accuracy: 0.1781\n",
      "Epoch 8/50\n",
      "3363/3363 [==============================] - 100s 30ms/step - loss: 4.5118 - accuracy: 0.1789\n",
      "Epoch 9/50\n",
      "3363/3363 [==============================] - 101s 30ms/step - loss: 4.4685 - accuracy: 0.1786\n",
      "Epoch 10/50\n",
      "3363/3363 [==============================] - 100s 30ms/step - loss: 4.4378 - accuracy: 0.1773\n",
      "Epoch 11/50\n",
      "3363/3363 [==============================] - 100s 30ms/step - loss: 4.4150 - accuracy: 0.1768\n",
      "Epoch 12/50\n",
      "3363/3363 [==============================] - 433s 129ms/step - loss: 4.3994 - accuracy: 0.1771\n",
      "Epoch 13/50\n",
      "3363/3363 [==============================] - 113s 34ms/step - loss: 4.3851 - accuracy: 0.1769\n",
      "Epoch 14/50\n",
      "3363/3363 [==============================] - 113s 34ms/step - loss: 4.3738 - accuracy: 0.1773\n",
      "Epoch 15/50\n",
      "3363/3363 [==============================] - 116s 35ms/step - loss: 4.3651 - accuracy: 0.1757\n",
      "Epoch 16/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.3581 - accuracy: 0.1772\n",
      "Epoch 17/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.3520 - accuracy: 0.1773\n",
      "Epoch 18/50\n",
      "3363/3363 [==============================] - 113s 34ms/step - loss: 4.3463 - accuracy: 0.1759\n",
      "Epoch 19/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.3419 - accuracy: 0.1773\n",
      "Epoch 20/50\n",
      "3363/3363 [==============================] - 116s 34ms/step - loss: 4.3368 - accuracy: 0.1769\n",
      "Epoch 21/50\n",
      "3363/3363 [==============================] - 116s 35ms/step - loss: 4.3325 - accuracy: 0.1764\n",
      "Epoch 22/50\n",
      "3363/3363 [==============================] - 126s 37ms/step - loss: 4.3294 - accuracy: 0.1758\n",
      "Epoch 23/50\n",
      "3363/3363 [==============================] - 148s 44ms/step - loss: 4.3264 - accuracy: 0.1762\n",
      "Epoch 24/50\n",
      "3363/3363 [==============================] - 133s 40ms/step - loss: 4.3243 - accuracy: 0.1770\n",
      "Epoch 25/50\n",
      "3363/3363 [==============================] - 150s 45ms/step - loss: 4.3211 - accuracy: 0.1777\n",
      "Epoch 26/50\n",
      "3363/3363 [==============================] - 159s 47ms/step - loss: 4.3180 - accuracy: 0.1778\n",
      "Epoch 27/50\n",
      "3363/3363 [==============================] - 162s 48ms/step - loss: 4.3167 - accuracy: 0.1775\n",
      "Epoch 28/50\n",
      "3363/3363 [==============================] - 166s 49ms/step - loss: 4.3134 - accuracy: 0.1770\n",
      "Epoch 29/50\n",
      "3363/3363 [==============================] - 140s 42ms/step - loss: 4.3120 - accuracy: 0.1774\n",
      "Epoch 30/50\n",
      "3363/3363 [==============================] - 141s 42ms/step - loss: 4.3108 - accuracy: 0.1772\n",
      "Epoch 31/50\n",
      "3363/3363 [==============================] - 129s 38ms/step - loss: 4.3088 - accuracy: 0.1770\n",
      "Epoch 32/50\n",
      "3363/3363 [==============================] - 121s 36ms/step - loss: 4.3068 - accuracy: 0.1776\n",
      "Epoch 33/50\n",
      "3363/3363 [==============================] - 132s 39ms/step - loss: 4.3051 - accuracy: 0.1780\n",
      "Epoch 34/50\n",
      "3363/3363 [==============================] - 118s 35ms/step - loss: 4.3039 - accuracy: 0.1779\n",
      "Epoch 35/50\n",
      "3363/3363 [==============================] - 114s 34ms/step - loss: 4.3019 - accuracy: 0.1776\n",
      "Epoch 36/50\n",
      "3363/3363 [==============================] - 114s 34ms/step - loss: 4.3006 - accuracy: 0.1788\n",
      "Epoch 37/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.2991 - accuracy: 0.1780\n",
      "Epoch 38/50\n",
      "3363/3363 [==============================] - 116s 34ms/step - loss: 4.2976 - accuracy: 0.1780\n",
      "Epoch 39/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.2965 - accuracy: 0.1782\n",
      "Epoch 40/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.2955 - accuracy: 0.1780\n",
      "Epoch 41/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.2946 - accuracy: 0.1776\n",
      "Epoch 42/50\n",
      "3363/3363 [==============================] - 115s 34ms/step - loss: 4.2923 - accuracy: 0.1787\n",
      "Epoch 43/50\n",
      "3363/3363 [==============================] - 112s 33ms/step - loss: 4.2922 - accuracy: 0.1788\n",
      "Epoch 44/50\n",
      "3363/3363 [==============================] - 120s 36ms/step - loss: 4.2909 - accuracy: 0.1786\n",
      "Epoch 45/50\n",
      "3363/3363 [==============================] - 122s 36ms/step - loss: 4.2900 - accuracy: 0.1777\n",
      "Epoch 46/50\n",
      "3363/3363 [==============================] - 120s 36ms/step - loss: 4.2886 - accuracy: 0.1784\n",
      "Epoch 47/50\n",
      "3363/3363 [==============================] - 121s 36ms/step - loss: 4.2879 - accuracy: 0.1782\n",
      "Epoch 48/50\n",
      "3363/3363 [==============================] - 119s 35ms/step - loss: 4.2869 - accuracy: 0.1792\n",
      "Epoch 49/50\n",
      "3363/3363 [==============================] - 122s 36ms/step - loss: 4.2861 - accuracy: 0.1782\n",
      "Epoch 50/50\n",
      "3363/3363 [==============================] - 125s 37ms/step - loss: 4.2838 - accuracy: 0.1787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c08aa534c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e65aa",
   "metadata": {},
   "source": [
    "#  Generating the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35099ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_sentence(seed_text, model, tokenizer, max_sequence_len, num_words=20):\n",
    "    generated_text = seed_text\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_index = np.argmax(predicted_probs)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                predictions.append(output_word)\n",
    "                break\n",
    "        generated_text += \" \" + output_word\n",
    "\n",
    "    return generated_text, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75fd1dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not a lack of love, but a lack of\n",
      "Associated Predictions: ['the', 'other', 'was', 'a', 'very']\n",
      "Generated Text: It is not a lack of love, but a lack of the other was a very\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"It is not a lack of love, but a lack of\"\n",
    "generated_text, predictions = generate_next_sentence(seed_text, model, tokenizer, max_sequence_len, num_words=5)\n",
    "\n",
    "print(seed_text)\n",
    "print(\"Associated Predictions:\", predictions)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
